---
phase: v2.0-p04-voice-pipeline
plan: 04
type: execute
wave: 3
depends_on: ["v2.0-04-02", "v2.0-04-03"]
files_modified:
  - livos/packages/ui/src/routes/ai-chat/voice-button.tsx
  - livos/packages/ui/src/routes/ai-chat/index.tsx
  - livos/packages/ui/src/routes/settings/voice.tsx
  - livos/packages/ui/src/routes/settings/_components/settings-content.tsx
  - nexus/packages/core/src/voice/voice-session.ts
  - nexus/packages/core/src/api.ts
autonomous: false

must_haves:
  truths:
    - "User can click a push-to-talk button in the AI chat to speak to the AI"
    - "Browser captures microphone audio via MediaRecorder and streams it over WebSocket"
    - "User hears the AI's spoken response in real-time via AudioContext playback"
    - "Interim transcripts appear in the chat UI while the user is speaking"
    - "User can configure Deepgram and Cartesia API keys in Settings"
    - "Voice mode only activates when both Deepgram and Cartesia API keys are configured"
    - "Latency timestamps are logged at each pipeline stage"
  artifacts:
    - path: "livos/packages/ui/src/routes/ai-chat/voice-button.tsx"
      provides: "VoiceButton component with push-to-talk, WebSocket connection, MediaRecorder capture, AudioContext playback"
      min_lines: 150
    - path: "livos/packages/ui/src/routes/settings/voice.tsx"
      provides: "Voice settings panel with Deepgram/Cartesia API key inputs, voice selection"
      min_lines: 60
  key_links:
    - from: "livos/packages/ui/src/routes/ai-chat/voice-button.tsx"
      to: "/ws/voice WebSocket endpoint"
      via: "new WebSocket() connection with JWT auth"
      pattern: "ws://.*ws/voice|wss://.*ws/voice"
    - from: "livos/packages/ui/src/routes/ai-chat/index.tsx"
      to: "livos/packages/ui/src/routes/ai-chat/voice-button.tsx"
      via: "VoiceButton rendered in chat input area"
      pattern: "VoiceButton"
    - from: "livos/packages/ui/src/routes/settings/_components/settings-content.tsx"
      to: "livos/packages/ui/src/routes/settings/voice.tsx"
      via: "Lazy-loaded VoiceContent section in settings"
      pattern: "VoiceContent"
---

<objective>
Build the Voice UI components and instrumentation. A push-to-talk button in the AI chat captures microphone audio, streams it over WebSocket to the voice pipeline, and plays back TTS audio in real-time. API key configuration in Settings enables/disables voice mode. Latency timestamps track the full pipeline.

Purpose: This is the user-facing layer that ties the entire voice pipeline together. Without the UI, users cannot interact with the voice features built in Plans 01-03.

Output: VoiceButton component, Voice settings panel, latency instrumentation, AI chat integration.
</objective>

<execution_context>
@C:\Users\hello\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\hello\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/v2.0-p04-voice-pipeline/v2.0-04-01-SUMMARY.md
@.planning/phases/v2.0-p04-voice-pipeline/v2.0-04-02-SUMMARY.md
@.planning/phases/v2.0-p04-voice-pipeline/v2.0-04-03-SUMMARY.md

Key reference files:
@livos/packages/ui/src/routes/ai-chat/index.tsx — AI chat page (add VoiceButton here)
@livos/packages/ui/src/routes/settings/_components/settings-content.tsx — Settings page (add Voice section)
@livos/packages/ui/src/routes/settings/gmail.tsx — Gmail settings (pattern for lazy-loaded settings panel)
@livos/packages/ui/src/routes/settings/webhooks.tsx — Webhooks settings (pattern for API key management)
</context>

<tasks>

<task type="auto">
  <name>Task 1: VoiceButton component + Voice settings panel + API endpoint</name>
  <files>
    livos/packages/ui/src/routes/ai-chat/voice-button.tsx
    livos/packages/ui/src/routes/settings/voice.tsx
    livos/packages/ui/src/routes/settings/_components/settings-content.tsx
    nexus/packages/core/src/api.ts
  </files>
  <action>
1. **Add voice config REST endpoints in api.ts:**
   - Add `GET /api/voice/config` route (after requireApiKey middleware):
     - Read voice config from Redis: `redis.get('nexus:config')` -> parse -> return `voice` section.
     - Return `{ enabled, hasDeepgramKey: !!config.voice?.deepgramApiKey, hasCartesiaKey: !!config.voice?.cartesiaApiKey, cartesiaVoiceId, sttLanguage, sttModel }`.
     - Do NOT return actual API key values (security).
   - Add `PUT /api/voice/config` route:
     - Accept body: `{ deepgramApiKey?, cartesiaApiKey?, cartesiaVoiceId?, sttLanguage?, sttModel?, enabled? }`.
     - Read current config from Redis, merge voice section, write back.
     - If keys are changed, publish `nexus:config:updated` Redis event so VoiceGateway can hot-reload.
     - Return updated config (without raw keys).

2. **Create livos/packages/ui/src/routes/settings/voice.tsx:**
   Follow the same pattern as `gmail.tsx` and `webhooks.tsx` for lazy-loaded settings panels.

   - Export `VoiceContent` component.
   - State: `deepgramApiKey`, `cartesiaApiKey`, `cartesiaVoiceId`, `sttLanguage`, `sttModel`, `enabled`, `loading`, `saving`, `hasDeepgramKey`, `hasCartesiaKey`.
   - On mount: Fetch `GET /api/voice/config` (via the existing Nexus API proxy pattern used in other settings). Populate state.
   - UI layout (using existing Tailwind/shadcn patterns):
     - **Enable Voice** toggle (enabled/disabled).
     - **Deepgram API Key** input (password type, with eye toggle for show/hide). Show green checkmark if `hasDeepgramKey` is true. Placeholder: "sk-..." if key already set.
     - **Cartesia API Key** input (same pattern).
     - **Voice ID** input with default value. Text field.
     - **STT Language** select dropdown: en, tr, de, fr, es, etc.
     - **STT Model** select: nova-3, nova-2.
     - **Save** button that PUTs to `/api/voice/config`.
     - Info card: "Voice mode requires both Deepgram (STT) and Cartesia (TTS) API keys. Get keys at deepgram.com and cartesia.ai."
   - Use the same styling patterns as other settings sections (Card, Input, Button from shadcn-components).

3. **Register Voice section in settings-content.tsx:**
   - Add lazy import at the top (near other lazy imports):
     ```typescript
     const VoiceContentLazy = React.lazy(() =>
       import('@/routes/settings/voice').then((m) => ({default: m.VoiceContent})),
     )
     ```
   - Add to the `settingsItems` array (or wherever sections are defined):
     ```typescript
     {id: 'voice', icon: TbMicrophone, label: 'Voice', description: 'Push-to-talk voice mode'}
     ```
     Import `TbMicrophone` from 'react-icons/tb'.
   - Add the section renderer in the switch/conditional (where GmailSection, WebhooksSection, etc. are rendered):
     ```typescript
     case 'voice':
       return <VoiceSection />
     ```
   - Add `VoiceSection` component at the bottom (same pattern as GmailSection):
     ```typescript
     function VoiceSection() {
       return (
         <Suspense fallback={<Loader2 className="animate-spin" />}>
           <VoiceContentLazy />
         </Suspense>
       )
     }
     ```

4. **Create livos/packages/ui/src/routes/ai-chat/voice-button.tsx:**
   This is the core UI component. It handles:
   - WebSocket connection to `/ws/voice`
   - Microphone capture via `navigator.mediaDevices.getUserMedia({ audio: { sampleRate: 16000, channelCount: 1, echoCancellation: true, noiseSuppression: true } })`
   - Audio encoding via MediaRecorder (format: 'audio/webm;codecs=opus' for browser compatibility, server-side Deepgram handles decoding) OR use AudioWorklet to send raw PCM. **Use MediaRecorder** for simplicity — Deepgram accepts webm/opus directly if we set encoding=opus in the Deepgram URL.

   **UPDATE for Deepgram compatibility:** Actually, the simplest approach is to use MediaRecorder with `mimeType: 'audio/webm;codecs=opus'` and tell Deepgram `encoding=opus&container=webm`. Deepgram handles the container format natively.

   **Component: `VoiceButton`**
   - Props: `{ disabled?: boolean, onTranscript?: (text: string) => void }`
   - State: `isConnected`, `isListening`, `isProcessing`, `isSpeaking`, `interimText`, `latencyData`.
   - Refs: `wsRef`, `mediaRecorderRef`, `audioContextRef`, `sourceBufferRef`.

   **WebSocket Connection:**
   - On component mount (if voice is enabled), establish WebSocket connection:
     ```typescript
     const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
     const wsUrl = `${protocol}//${window.location.host}/ws/voice?token=${jwt}`;
     ```
     Where `jwt` comes from the existing auth context (cookie or localStorage — follow existing pattern in trpc client).
   - Handle messages:
     - Text messages (JSON): Parse control messages.
       - `{ type: 'connected', sessionId }` -> set isConnected.
       - `{ type: 'transcript', text, isFinal }` -> update interimText. If isFinal, call onTranscript.
       - `{ type: 'state-change', to }` -> update state flags.
       - `{ type: 'tts-done' }` -> set isSpeaking to false.
       - `{ type: 'latency', ... }` -> update latencyData.
       - `{ type: 'error', message }` -> show toast or inline error.
     - Binary messages: TTS audio data -> feed to AudioContext for playback.

   **Push-to-Talk:**
   - Button: Round microphone icon button. Shows different states via color:
     - Gray: idle (not connected or voice not available)
     - Blue pulse: connected, ready
     - Red pulse: listening (recording)
     - Yellow: processing (waiting for AI)
     - Green pulse: speaking (TTS playing)
   - On mousedown/touchstart: Send `{ type: 'start-listening' }` control message, start MediaRecorder.
   - On mouseup/touchend: Stop MediaRecorder, send `{ type: 'stop-listening' }` control message.
   - MediaRecorder `ondataavailable`: Send data as binary frame via WebSocket.
     - Set `timeslice: 250` in MediaRecorder.start(250) to get chunks every 250ms for real-time streaming.

   **Audio Playback (TTS):**
   - Use `AudioContext` with a queue-based approach:
     - Maintain a queue of audio chunks (Buffer/ArrayBuffer).
     - Use `AudioContext.decodeAudioData()` or raw PCM playback via AudioWorklet.
     - **For raw PCM from Cartesia (pcm_s16le at 24000Hz):** Create an AudioWorklet that reads PCM samples from a ring buffer.
     - Simpler approach: Use `AudioBufferSourceNode` per chunk. Queue chunks and play sequentially.
     - **Recommended approach:** Accumulate PCM chunks into an AudioBuffer, create AudioBufferSourceNode, connect to destination, play. Chain with `onended` for sequential playback.
   - Playback implementation:
     ```typescript
     const audioCtx = new AudioContext({ sampleRate: 24000 });
     const audioQueue: ArrayBuffer[] = [];
     let isPlaying = false;

     function enqueueAudio(pcmData: ArrayBuffer) {
       audioQueue.push(pcmData);
       if (!isPlaying) playNext();
     }

     function playNext() {
       if (audioQueue.length === 0) { isPlaying = false; return; }
       isPlaying = true;
       const pcm = audioQueue.shift()!;
       const float32 = new Float32Array(pcm.byteLength / 2);
       const int16 = new Int16Array(pcm);
       for (let i = 0; i < int16.length; i++) float32[i] = int16[i] / 32768;
       const buffer = audioCtx.createBuffer(1, float32.length, 24000);
       buffer.getChannelData(0).set(float32);
       const source = audioCtx.createBufferSource();
       source.buffer = buffer;
       source.connect(audioCtx.destination);
       source.onended = () => playNext();
       source.start();
     }
     ```

   **Reconnection:**
   - On WebSocket close (not user-initiated): Reconnect with exponential backoff (1s, 2s, 4s, 8s, max 16s).
   - Max 5 reconnect attempts before giving up and showing error.

   **Voice Availability Check:**
   - On mount, check if voice is configured: Fetch GET /api/voice/config. If `hasDeepgramKey && hasCartesiaKey && enabled`, show the button. Otherwise hide it.
   - Re-check when settings change (optional: use polling every 30s or event).

5. **IMPORTANT: Update DeepgramRelay config for browser audio format:**
   In Plan 02's DeepgramRelay, the encoding was set to `linear16`. Since the browser sends webm/opus via MediaRecorder, update the VoiceGateway to detect the audio format. The simplest approach: configure Deepgram with `encoding=opus&container=webm` when the browser is the source. Add a parameter to VoiceSession for the audio format hint.
   - In voice-session.ts, when `start-listening` is received, check for an optional `format` field: `{ type: 'start-listening', format: 'webm-opus' }`.
   - Pass this format to DeepgramRelay config. If format is 'webm-opus', use `encoding: 'opus'` and add `container: 'webm'` to the Deepgram URL params.
   - Default to 'linear16' if no format specified.
  </action>
  <verify>
    - `npx tsc --noEmit` passes for both livos and nexus packages.
    - VoiceButton component renders a microphone button in the AI chat.
    - Voice settings panel is accessible in Settings under "Voice" section.
    - GET/PUT /api/voice/config endpoints respond correctly.
    - VoiceButton handles push-to-talk interaction (mousedown/mouseup).
  </verify>
  <done>
    VoiceButton component captures microphone audio via MediaRecorder, streams to /ws/voice WebSocket, receives TTS audio and plays it back via AudioContext. Voice settings panel allows API key configuration. Voice mode auto-detects availability.
  </done>
</task>

<task type="auto">
  <name>Task 2: Latency instrumentation + AI chat integration</name>
  <files>
    nexus/packages/core/src/voice/voice-session.ts
    livos/packages/ui/src/routes/ai-chat/voice-button.tsx
    livos/packages/ui/src/routes/ai-chat/index.tsx
  </files>
  <action>
1. **Add latency timestamps to VoiceSession:**
   - Add a `LatencyTracker` utility in VoiceSession (inline, not a separate file):
     ```typescript
     interface PipelineTimestamps {
       micCapture?: number;      // When browser started recording
       serverReceive?: number;   // When first audio chunk arrived at server
       sttStart?: number;        // When audio was first sent to Deepgram
       sttTranscript?: number;   // When final transcript was received
       llmStart?: number;        // When daemon.addToInbox was called
       llmFirstToken?: number;   // When first text chunk arrived from AI
       ttsStart?: number;        // When first text was sent to Cartesia
       ttsFirstAudio?: number;   // When first audio chunk came back from Cartesia
       browserPlayback?: number; // When browser started playing audio (reported by client)
     }
     ```
   - Track timestamps at each stage:
     - `serverReceive`: Set in `handleBinaryMessage` on first audio chunk per utterance.
     - `sttStart`: Set when first chunk is sent to DeepgramRelay.
     - `sttTranscript`: Set when final transcript is received.
     - `llmStart`: Set when onTranscript calls addToInbox.
     - `ttsStart`: Set when first text is sent to CartesiaRelay.
     - `ttsFirstAudio`: Set on first CartesiaRelay 'audio' event.
   - When TTS completes (or at 'tts-done'), send latency data to browser:
     ```typescript
     this.sendControl({
       type: 'latency',
       timestamps: this.currentTimestamps,
       durations: {
         sttMs: sttTranscript - serverReceive,
         llmMs: llmFirstToken - sttTranscript,
         ttsMs: ttsFirstAudio - ttsStart,
         totalServerMs: ttsFirstAudio - serverReceive,
       }
     });
     ```
   - Reset timestamps on each new utterance (when start-listening is received).
   - Log full pipeline latency at info level: `logger.info('Voice pipeline latency', { durations })`.

2. **Client-side latency tracking in VoiceButton:**
   - Track `micCapture` timestamp when MediaRecorder starts.
   - Send `{ type: 'start-listening', micCapture: Date.now(), format: 'webm-opus' }` so server knows when mic started.
   - Track `browserPlayback` when first audio chunk is played.
   - On receiving `{ type: 'latency', ... }`: Calculate end-to-end:
     ```
     e2e = browserPlayback - micCapture
     ```
   - Display latency info: Show small "XXXms" text below the voice button when latency data is available. Fade out after 5 seconds.
   - Console.log full pipeline breakdown for debugging.

3. **Integrate VoiceButton into AI chat page (index.tsx):**
   - Import VoiceButton (lazy load to avoid MediaRecorder import for non-voice users):
     ```typescript
     const VoiceButton = lazy(() => import('./voice-button').then(m => ({ default: m.VoiceButton })));
     ```
   - In the chat input area (where the send button is), add VoiceButton next to the send button:
     ```tsx
     <Suspense fallback={null}>
       <VoiceButton
         onTranscript={(text) => {
           // Optionally add transcript to chat as a user message for visual feedback
           // The actual AI processing is handled server-side via addToInbox
         }}
       />
     </Suspense>
     ```
   - Position: Left of the send button, same row. Only visible when voice is configured (VoiceButton handles its own visibility check).
   - When a voice transcript comes in, optionally display it as a user message bubble in the chat (grayed out, with a microphone icon to indicate it was spoken, not typed). This provides visual feedback even though the actual processing happens on the server side.
  </action>
  <verify>
    - `npx tsc --noEmit` passes for both packages.
    - VoiceButton appears in the AI chat input area.
    - Latency timestamps are logged in VoiceSession.
    - Latency control message is sent to browser on TTS completion.
    - VoiceButton shows latency indicator after a voice interaction.
  </verify>
  <done>
    Full latency instrumentation at every pipeline stage (mic -> server -> STT -> LLM -> TTS -> playback). VoiceButton is integrated into the AI chat page with push-to-talk interaction. Voice transcripts appear in chat. Latency is displayed to the user.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Complete voice pipeline: push-to-talk button in AI chat, microphone capture, WebSocket streaming, Deepgram STT, AI processing, Cartesia TTS, real-time audio playback. Voice API key settings in Settings page. Latency instrumentation.
  </what-built>
  <how-to-verify>
    1. Open Settings -> Voice section. Enter Deepgram and Cartesia API keys. Click Save.
    2. Open AI Chat. Verify a microphone button appears next to the send button.
    3. Press and hold the microphone button. Speak a question (e.g., "What is the weather?").
    4. Verify: interim transcript text appears in the UI while speaking.
    5. Release the button. Verify: the AI processes the transcript and responds.
    6. Verify: you hear the AI's response spoken aloud in real-time (not after full response).
    7. Check the latency indicator below the voice button — should show total pipeline time.
    8. Test error cases: Remove one API key in Settings, verify voice button is hidden or disabled.
    9. Test reconnection: Refresh the page, verify voice reconnects automatically.
  </how-to-verify>
  <resume-signal>Type "approved" if voice pipeline works end-to-end, or describe issues to fix.</resume-signal>
</task>

</tasks>

<verification>
1. TypeScript compiles for both livos and nexus: zero errors.
2. VoiceButton renders in AI chat: visible microphone button.
3. Push-to-talk works: mousedown starts recording, mouseup stops.
4. Audio streams to server: WebSocket binary frames flow on recording.
5. Transcript appears: interim text visible in UI during speech.
6. AI responds: transcript processed by agent, response generated.
7. Audio playback: TTS audio heard in real-time through browser speakers.
8. Latency logged: server logs show pipeline timing at each stage.
9. Settings work: API keys saved and loaded, voice mode toggles correctly.
10. Reconnection: WebSocket reconnects automatically on disconnect.
</verification>

<success_criteria>
- User can press and hold microphone button to speak, release to send.
- Interim transcripts display in real-time as user speaks.
- AI response is heard as spoken audio, streaming word-by-word.
- Deepgram and Cartesia API keys are configurable in Settings.
- Voice button only appears when both API keys are configured.
- Latency is tracked at every pipeline stage and displayed to user.
- WebSocket reconnects automatically on network interruption.
- End-to-end latency target: p95 < 1200ms.
</success_criteria>

<output>
After completion, create `.planning/phases/v2.0-p04-voice-pipeline/v2.0-04-04-SUMMARY.md`
</output>
