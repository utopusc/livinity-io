---
phase: v2.0-p04-voice-pipeline
plan: 03
type: execute
wave: 2
depends_on: ["v2.0-04-01"]
files_modified:
  - nexus/packages/core/src/voice/cartesia-relay.ts
  - nexus/packages/core/src/voice/voice-session.ts
  - nexus/packages/core/src/voice/index.ts
  - nexus/packages/core/src/daemon.ts
autonomous: true

must_haves:
  truths:
    - "AI response text is streamed to Cartesia TTS via WebSocket and audio chunks are sent back to the browser in real-time"
    - "The user hears the AI response word-by-word as audio is generated, not after the full response is complete"
    - "Cartesia context_id is used for continuity within a voice session (consistent voice characteristics)"
    - "VoiceSession transitions from 'processing' to 'speaking' when TTS audio starts flowing"
    - "VoiceSession transitions from 'speaking' to 'idle' when TTS completes"
  artifacts:
    - path: "nexus/packages/core/src/voice/cartesia-relay.ts"
      provides: "CartesiaRelay class — WebSocket connection to Cartesia TTS, text-to-speech streaming, audio chunk relay"
      min_lines: 120
  key_links:
    - from: "nexus/packages/core/src/daemon.ts"
      to: "nexus/packages/core/src/voice/voice-session.ts"
      via: "Agent response text emitted to voice session for TTS synthesis"
      pattern: "voiceSessionId|voice-response"
    - from: "nexus/packages/core/src/voice/voice-session.ts"
      to: "nexus/packages/core/src/voice/cartesia-relay.ts"
      via: "VoiceSession calls cartesiaRelay.synthesize(text) to generate speech"
      pattern: "CartesiaRelay"
    - from: "nexus/packages/core/src/voice/cartesia-relay.ts"
      to: "browser WebSocket"
      via: "Audio chunks from Cartesia relayed to browser via VoiceSession.sendAudio()"
      pattern: "sendAudio"
---

<objective>
Integrate Cartesia real-time TTS into the voice pipeline. AI response text flows from the daemon through CartesiaRelay to Cartesia's WebSocket API, and synthesized audio chunks are relayed back to the browser for immediate playback.

Purpose: This is the "voice" of the AI. Without TTS, the AI can only respond with text. Real-time streaming ensures the user hears the response as it is generated, minimizing perceived latency.

Output: CartesiaRelay class, VoiceSession TTS wiring, daemon response-to-voice integration.
</objective>

<execution_context>
@C:\Users\hello\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\hello\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/v2.0-p04-voice-pipeline/v2.0-04-01-SUMMARY.md

Key reference files:
@nexus/packages/core/src/voice/voice-session.ts — VoiceSession state machine (Plans 01+02)
@nexus/packages/core/src/voice/index.ts — VoiceGateway (Plan 01)
@nexus/packages/core/src/voice/deepgram-relay.ts — DeepgramRelay pattern (Plan 02)
@nexus/packages/core/src/daemon.ts — processInboxItem, agent execution, response routing
@nexus/packages/core/src/sdk-agent-runner.ts — SdkAgentRunner with streaming events
</context>

<tasks>

<task type="auto">
  <name>Task 1: CartesiaRelay class — streaming TTS WebSocket</name>
  <files>
    nexus/packages/core/src/voice/cartesia-relay.ts
  </files>
  <action>
1. **Create nexus/packages/core/src/voice/cartesia-relay.ts:**

   Cartesia's real-time TTS API uses a WebSocket at `wss://api.cartesia.ai/tts/websocket?api_key=KEY&cartesia_version=2025-04-16`. Messages are JSON with audio returned as base64-encoded chunks in the response.

   - Import `WebSocket` from 'ws', `EventEmitter` from 'events', `logger` from '../logger.js'.

   - Define `CartesiaRelayConfig`:
     ```typescript
     interface CartesiaRelayConfig {
       apiKey: string;
       modelId?: string;      // default 'sonic-2'
       voiceId: string;       // UUID for the voice
       outputEncoding?: string; // default 'pcm_s16le' (raw PCM for AudioWorklet)
       sampleRate?: number;    // default 24000 (Cartesia default)
       language?: string;      // default 'en'
     }
     ```

   - Implement `CartesiaRelay` class extending `EventEmitter`:
     - Events: `'audio'` (Buffer), `'done'`, `'error'`, `'open'`.
     - Constructor takes `CartesiaRelayConfig`.
     - Private fields: `ws: WebSocket | null`, `config`, `contextId: string` (UUID, persistent per session for voice continuity), `isClosing: boolean`.

     - `connect()`: Build WebSocket URL:
       ```
       wss://api.cartesia.ai/tts/websocket?api_key=${apiKey}&cartesia_version=2025-04-16
       ```
       - Create WebSocket.
       - `ws.on('open')`: Emit 'open', log info.
       - `ws.on('message')`: Parse JSON. Cartesia sends:
         ```json
         {
           "type": "chunk",
           "data": "<base64-encoded-audio>",
           "step_time": 0.05,
           "done": false
         }
         ```
         And when complete:
         ```json
         { "type": "done", "done": true }
         ```
         On `type: 'chunk'`: Decode base64 `data` field to Buffer, emit `'audio'` event.
         On `type: 'done'` or `done: true`: Emit `'done'` event.
         On `type: 'error'`: Emit `'error'` event with message.
       - `ws.on('close')`: Log close. If not isClosing, emit error.
       - `ws.on('error')`: Log, emit 'error'.

     - `synthesize(text: string, flush?: boolean)`: Send TTS request:
       ```json
       {
         "model_id": "sonic-2",
         "transcript": text,
         "voice": { "mode": "id", "id": voiceId },
         "output_format": {
           "container": "raw",
           "encoding": outputEncoding,
           "sample_rate": sampleRate
         },
         "language": language,
         "context_id": contextId,
         "continue": true,
         "add_timestamps": false
       }
       ```
       If `flush` is true, send a second message immediately after with `"transcript": " "` and `"continue": false` to flush the buffer.
       Use `ws.send(JSON.stringify(msg))`.

     - `flush()`: Send empty flush message to finalize any buffered audio:
       ```json
       {
         "model_id": "sonic-2",
         "transcript": " ",
         "voice": { "mode": "id", "id": voiceId },
         "output_format": { "container": "raw", "encoding": outputEncoding, "sample_rate": sampleRate },
         "language": language,
         "context_id": contextId,
         "continue": false
       }
       ```

     - `close()`: Set isClosing, close ws, remove listeners.
     - `isConnected()`: Returns readyState === OPEN.
     - `getContextId()`: Returns contextId.

   - Export `CartesiaRelay`, `CartesiaRelayConfig`.
  </action>
  <verify>
    - `npx tsc --noEmit` passes.
    - CartesiaRelay exports are importable.
    - grep "api.cartesia.ai" confirms correct endpoint.
    - grep "context_id" confirms continuity support.
  </verify>
  <done>
    CartesiaRelay class connects to Cartesia's TTS WebSocket, sends text for synthesis, receives base64-encoded audio chunks, decodes them to Buffer, and maintains context_id for voice continuity within a session.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire TTS into VoiceSession + daemon response routing</name>
  <files>
    nexus/packages/core/src/voice/voice-session.ts
    nexus/packages/core/src/voice/index.ts
    nexus/packages/core/src/daemon.ts
  </files>
  <action>
1. **Update VoiceSession to manage CartesiaRelay:**
   - Import `CartesiaRelay` from `./cartesia-relay.js`.
   - Add to constructor config: `cartesiaApiKey?: string`, `cartesiaVoiceId?: string`, `cartesiaModelId?: string`.
   - Add private field: `ttsRelay: CartesiaRelay | null = null`.

   - Add `startTts()` method:
     - Check cartesiaApiKey exists. If not, log warning and skip TTS.
     - Create CartesiaRelay with config.
     - Wire `ttsRelay.on('audio', (chunk: Buffer) => ...)`:
       - Call `this.sendAudio(chunk)` to relay audio to browser via WebSocket binary frame.
       - If state is 'processing', transition to 'speaking'.
     - Wire `ttsRelay.on('done', () => ...)`:
       - Send control message: `{ type: 'tts-done' }`.
       - Transition to 'idle'.
       - Close TTS relay.
     - Wire `ttsRelay.on('error', ...)`: Log, send error to browser.
     - Call `ttsRelay.connect()`.

   - Add `speakText(text: string, isFinal?: boolean)` method:
     - If ttsRelay is null or not connected, call `startTts()` first (lazy init).
     - Call `ttsRelay.synthesize(text)`.
     - If `isFinal`, call `ttsRelay.flush()` after a short delay (50ms) to ensure the last text is sent first.

   - **Update `close()`:** Close ttsRelay if exists.

2. **Update VoiceGateway to pass TTS config:**
   - When creating VoiceSession, pass:
     - `cartesiaApiKey: deps.voiceConfig?.cartesiaApiKey`
     - `cartesiaVoiceId: deps.voiceConfig?.cartesiaVoiceId`
     - `cartesiaModelId: deps.voiceConfig?.cartesiaModelId`

   - Expose `getSession(sessionId: string)` method on VoiceGateway to allow daemon to look up active voice sessions by ID.

3. **Update daemon.ts to route AI responses to active voice sessions:**
   - The key integration point: When the daemon processes a voice inbox item (source === 'voice'), the AI response text needs to flow to CartesiaRelay for TTS synthesis.

   - In `processInboxItem()`, after the agent produces a response:
     - Check if `item.source === 'voice'` (or check for `item.params?.voiceSessionId`).
     - If a voiceSessionId is present, emit a Redis pub/sub message on channel `nexus:voice:response`:
       ```json
       {
         "sessionId": "...",
         "text": "AI response text",
         "isFinal": true
       }
       ```
     - This pub/sub approach decouples daemon from VoiceGateway (daemon doesn't need a direct reference).

   - In VoiceGateway, subscribe to `nexus:voice:response` Redis channel:
     - On message: Look up VoiceSession by sessionId. If found, call `session.speakText(text, isFinal)`.
     - This requires a dedicated Redis subscriber connection (same pattern as WsGateway's redisSub).

   - **For streaming responses** (SdkAgentRunner emits text chunks via events):
     - In the daemon's agent event handler for voice source, intercept `'text_delta'` or `'text'` type events.
     - Buffer text until a sentence boundary (period, question mark, exclamation, or 100+ chars without punctuation).
     - When a sentence boundary is detected, publish partial text to `nexus:voice:response` with `isFinal: false`.
     - On `'done'` event, flush remaining buffer with `isFinal: true`.
     - This sentence-boundary buffering ensures natural-sounding TTS (Cartesia works best with complete sentences).

   - Add the Redis subscriber connection for voice to VoiceGatewayDeps:
     ```typescript
     export interface VoiceGatewayDeps {
       redis: Redis;
       redisSub: Redis;  // Dedicated subscriber for voice response channel
       daemon: Daemon;
       voiceConfig?: VoiceConfig;
     }
     ```

4. **Update index.ts to provide redisSub to VoiceGateway:**
   - Create another Redis duplicate for voice pub/sub:
     ```typescript
     const voiceRedisSub = redis.duplicate();
     ```
   - Pass to VoiceGateway constructor.
   - Close in shutdown: `await voiceRedisSub.quit().catch(() => {});`
  </action>
  <verify>
    - `npx tsc --noEmit` passes.
    - grep "CartesiaRelay" in voice-session.ts confirms TTS integration.
    - grep "nexus:voice:response" in daemon.ts and voice/index.ts confirms pub/sub wiring.
    - grep "speakText" in voice-session.ts confirms TTS method exists.
    - grep "voiceRedisSub" in index.ts confirms Redis subscriber for voice.
  </verify>
  <done>
    AI response text flows from daemon via Redis pub/sub to VoiceGateway, which routes it to the correct VoiceSession's CartesiaRelay for TTS synthesis. Audio chunks are relayed back to the browser in real-time. Sentence-boundary buffering ensures natural speech output. VoiceSession state transitions from processing to speaking to idle.
  </done>
</task>

</tasks>

<verification>
1. TypeScript compiles: `cd nexus/packages/core && npx tsc --noEmit` — zero errors.
2. CartesiaRelay connects to correct endpoint: grep "api.cartesia.ai" confirms URL.
3. TTS audio flow: CartesiaRelay 'audio' event -> VoiceSession.sendAudio() -> browser WebSocket binary frame.
4. Response routing: daemon publishes to nexus:voice:response -> VoiceGateway subscribes -> routes to VoiceSession.
5. Sentence buffering: Text accumulated until punctuation boundary before sending to TTS.
6. State transitions: processing -> speaking (first audio chunk) -> idle (TTS done).
7. Context continuity: context_id persists per VoiceSession for consistent voice.
</verification>

<success_criteria>
- CartesiaRelay class connects to Cartesia TTS WebSocket with API key authentication.
- Text is sent sentence-by-sentence for natural speech synthesis.
- Base64 audio chunks from Cartesia are decoded and relayed as binary frames to browser.
- VoiceSession manages TTS lifecycle (start, synthesize, flush, close).
- Daemon routes voice responses via Redis pub/sub (decoupled from VoiceGateway).
- context_id is maintained per session for voice continuity.
- Streaming: user hears audio as it is generated, not after full response completes.
</success_criteria>

<output>
After completion, create `.planning/phases/v2.0-p04-voice-pipeline/v2.0-04-03-SUMMARY.md`
</output>
