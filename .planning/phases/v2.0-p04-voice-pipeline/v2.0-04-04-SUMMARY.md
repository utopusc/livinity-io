---
phase: v2.0-p04-voice-pipeline
plan: 04
subsystem: ui, voice, api
tags: [voice, push-to-talk, webrtc, medirecorder, audiocontext, websocket, tts-playback, deepgram, cartesia, settings, trpc, latency]

# Dependency graph
requires:
  - phase: v2.0-04-02
    provides: DeepgramRelay STT integration and VoiceSession state machine
  - phase: v2.0-04-03
    provides: CartesiaRelay TTS streaming and daemon response routing
provides:
  - VoiceButton push-to-talk component with WebSocket streaming and AudioContext playback
  - Voice settings panel (Deepgram/Cartesia API key management, voice selection, STT config)
  - GET/PUT /api/voice/config REST endpoints
  - tRPC proxy routes (getVoiceConfig, updateVoiceConfig)
  - PipelineTimestamps latency instrumentation (STT/LLM/TTS/E2E)
  - AI chat VoiceButton integration with transcript-to-message injection
affects: [deployment, v2.0-p05, v2.0-p06]

# Tech tracking
tech-stack:
  added: []
  patterns:
    - "AudioPlaybackQueue: sequential PCM s16le playback via AudioContext at 24kHz"
    - "Push-to-talk via onPointerDown/onPointerUp (works on desktop and mobile)"
    - "MediaRecorder with webm/opus 250ms timeslice for real-time audio streaming"
    - "Voice availability gating via trpcReact.ai.getVoiceConfig polling (30s)"
    - "Exponential backoff WebSocket reconnection (1s-16s, max 5 attempts)"
    - "tRPC proxy for voice config: UI -> livinityd -> fetch to Nexus REST API"

key-files:
  created:
    - livos/packages/ui/src/routes/ai-chat/voice-button.tsx
    - livos/packages/ui/src/routes/settings/voice.tsx
  modified:
    - nexus/packages/core/src/api.ts
    - nexus/packages/core/src/voice/voice-session.ts
    - livos/packages/livinityd/source/modules/ai/routes.ts
    - livos/packages/ui/src/routes/settings/_components/settings-content.tsx
    - livos/packages/ui/src/routes/ai-chat/index.tsx

key-decisions:
  - "WebSocket JWT auth via query param (?token=) consistent with existing WsGateway pattern"
  - "MediaRecorder webm/opus format with Deepgram encoding=opus (no server-side re-encoding needed)"
  - "AudioPlaybackQueue chains AudioBufferSourceNode.onended for gapless sequential playback"
  - "VoiceButton renders null when voice not configured (zero UI impact for non-voice users)"
  - "tRPC proxy routes for voice config (consistent with existing settings pattern, not direct fetch)"
  - "Latency data sent as control message after TTS completion, with server-side durations and client-side E2E"

patterns-established:
  - "Voice settings pattern: REST endpoint -> tRPC proxy -> React settings panel with masked API keys"
  - "Audio playback pattern: PCM s16le chunks queued and played via AudioBufferSourceNode chain"
  - "Push-to-talk pattern: pointerDown starts MediaRecorder + WS control, pointerUp stops + sends stop-listening"
  - "Pipeline latency pattern: PipelineTimestamps struct reset per utterance, durations calculated at tts-done"

# Metrics
duration: 8min
completed: 2026-02-20
---

# Phase 4 Plan 04: Voice UI + Latency Instrumentation Summary

**Push-to-talk VoiceButton with MediaRecorder capture, WebSocket streaming, AudioContext TTS playback, voice settings panel, and full pipeline latency instrumentation**

## Performance

- **Duration:** 8 min
- **Started:** 2026-02-20T16:10:00Z
- **Completed:** 2026-02-20T16:18:04Z
- **Tasks:** 2 (+ 1 checkpoint skipped per standing instruction)
- **Files modified:** 7

## Accomplishments
- VoiceButton component (466 lines) with full push-to-talk lifecycle: connect, record, stream, receive, playback
- Voice settings panel with Deepgram/Cartesia API key management, voice selection, STT language/model config
- REST + tRPC API for voice configuration (GET/PUT with masked keys for security)
- PipelineTimestamps latency tracking at every stage (mic -> server -> STT -> LLM -> TTS -> playback)
- VoiceButton integrated into AI chat with transcript injection as user messages
- Browser audio format support (webm/opus from MediaRecorder -> Deepgram encoding=opus)

## Task Commits

Each task was committed atomically:

1. **Task 1: VoiceButton component + Voice settings panel + API endpoints** - `9b9b636` (feat)
2. **Task 2: Latency instrumentation + AI chat integration** - `eea132d` (feat)

## Files Created/Modified

- `livos/packages/ui/src/routes/ai-chat/voice-button.tsx` - VoiceButton component: WebSocket connection, MediaRecorder capture, AudioPlaybackQueue, push-to-talk, visual states, latency display, auto-reconnect
- `livos/packages/ui/src/routes/settings/voice.tsx` - VoiceContent settings panel: Deepgram/Cartesia API key inputs, voice ID, STT language/model, enable toggle, status card
- `livos/packages/ui/src/routes/settings/_components/settings-content.tsx` - Registered Voice section in settings menu (TbMicrophone icon, lazy-loaded VoiceContentLazy)
- `nexus/packages/core/src/api.ts` - GET/PUT /api/voice/config REST endpoints (masked API keys in response)
- `nexus/packages/core/src/voice/voice-session.ts` - PipelineTimestamps interface, latency tracking at each pipeline stage, sendLatencyData() method, webm-opus format support
- `livos/packages/livinityd/source/modules/ai/routes.ts` - getVoiceConfig/updateVoiceConfig tRPC proxy routes
- `livos/packages/ui/src/routes/ai-chat/index.tsx` - Lazy-loaded VoiceButton in chat input area with onTranscript callback

## Decisions Made

1. **WebSocket auth via query param**: Used `?token=${jwt}` pattern consistent with existing WsGateway, since WebSocket API doesn't support custom headers in the browser.

2. **MediaRecorder webm/opus format**: Browser MediaRecorder outputs webm/opus natively. Configured Deepgram with `encoding: 'opus'` to decode directly, avoiding server-side audio re-encoding.

3. **AudioPlaybackQueue sequential playback**: PCM s16le chunks from Cartesia (24kHz) played via chained AudioBufferSourceNode.onended callbacks for gapless sequential playback without gaps or overlaps.

4. **VoiceButton self-hides when unconfigured**: Returns null if `!isConfigured || state === 'unavailable'`, so non-voice users see no UI change whatsoever.

5. **tRPC proxy routes for voice config**: Followed existing pattern (Gmail, webhooks) where UI uses tRPC through livinityd which proxies to Nexus REST API. Consistent architecture, no direct fetch from UI to Nexus.

6. **Latency instrumentation at tts-done**: Server calculates durations (sttMs, llmMs, ttsMs, totalServerMs) when TTS completes, sends as control message. Client calculates E2E from micCapture to browserPlayback timestamps.

## Deviations from Plan

### Auto-fixed Issues

**1. [Rule 2 - Missing Critical] Added Deepgram encoding config for webm-opus format**
- **Found during:** Task 1 (VoiceSession format support)
- **Issue:** DeepgramRelay defaulted to `linear16` encoding. Browser MediaRecorder sends webm/opus. Without configuring Deepgram for opus encoding, STT would fail to decode audio.
- **Fix:** Added `format` field to `start-listening` control message. When format is `webm-opus`, DeepgramRelay is configured with `encoding: 'opus'` and `sampleRate: 48000` instead of default `linear16`/`16000`.
- **Files modified:** `nexus/packages/core/src/voice/voice-session.ts`
- **Verification:** TypeScript compiles, format parameter flows from browser through VoiceSession to DeepgramRelay config.
- **Committed in:** `9b9b636` (Task 1 commit)

**2. [Rule 1 - Bug] Fixed unreachable state comparison in VoiceButton**
- **Found during:** Task 1 (TypeScript verification)
- **Issue:** TS2367 error — `state !== 'unavailable'` comparison was unreachable after early `return null` for unavailable state. TypeScript correctly narrowed the type to exclude 'unavailable'.
- **Fix:** Removed the `showButton` variable and changed `stateStyles` to `Record<Exclude<VoiceButtonState, 'unavailable'>, string>` so TypeScript recognizes remaining states are valid.
- **Files modified:** `livos/packages/ui/src/routes/ai-chat/voice-button.tsx`
- **Verification:** `npx tsc --noEmit` passes with zero voice-related errors.
- **Committed in:** `9b9b636` (Task 1 commit)

---

**Total deviations:** 2 auto-fixed (1 missing critical, 1 bug)
**Impact on plan:** Both fixes essential for correctness. The Deepgram encoding config is required for browser audio to be recognized. The TypeScript fix was a straightforward type narrowing issue. No scope creep.

## Issues Encountered

- **Tab indentation in index.tsx**: The AI chat file uses actual tab characters, which caused multiple string replacement failures when trying to insert VoiceButton. Resolved by using unique substring matching that avoided leading whitespace.

## User Setup Required

Voice pipeline requires external API keys to be configured via the Settings > Voice panel:
- **Deepgram API key** (STT) — Get from https://console.deepgram.com/signup
- **Cartesia API key** (TTS) — Get from https://play.cartesia.ai/keys

No environment variable changes needed; keys are stored in Redis via the Settings UI.

## Next Phase Readiness

- **Voice Pipeline phase complete**: All 4 plans delivered — WebSocket gateway, Deepgram STT, Cartesia TTS, and Voice UI with latency instrumentation.
- **Ready for Phase 5** (LivHub Skill Registry): No dependencies on voice pipeline.
- **Deployment needed**: Voice features require deploying both nexus-core (API + voice gateway) and livos (UI) to server. PM2 restart of livos, nexus-core processes.
- **Concern**: Voice pipeline is end-to-end but untested with real API keys. First deployment will be the real integration test.

---
*Phase: v2.0-p04-voice-pipeline*
*Completed: 2026-02-20*
