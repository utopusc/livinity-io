---
phase: v1.5-01-provider-abstraction-claude
plan: 03
type: execute
wave: 3
depends_on: ["v1.5-01-01", "v1.5-01-02"]
files_modified:
  - nexus/packages/core/src/providers/gemini.ts
  - nexus/packages/core/src/providers/manager.ts
  - nexus/packages/core/src/providers/index.ts
  - nexus/packages/core/src/brain.ts
  - nexus/packages/core/src/index.ts
autonomous: true

must_haves:
  truths:
    - "User sends a message and receives a streaming response from Claude (Sonnet by default)"
    - "If Claude API returns 429/503/timeout, the system automatically retries with Gemini"
    - "Existing Gemini conversations continue to work identically after migration"
    - "Token usage is reported consistently regardless of which provider handled the request"
    - "The streaming experience (chunks, done events) is identical to before the migration"
    - "Brain class API is unchanged -- all existing callers (agent.ts, daemon.ts, api.ts, etc.) work without modification"
  artifacts:
    - path: "nexus/packages/core/src/providers/gemini.ts"
      provides: "GeminiProvider implementing AIProvider, extracted from current Brain class Gemini code"
      exports: ["GeminiProvider"]
      min_lines: 100
    - path: "nexus/packages/core/src/providers/manager.ts"
      provides: "ProviderManager that selects provider, executes with fallback, handles health checks"
      exports: ["ProviderManager"]
      min_lines: 80
    - path: "nexus/packages/core/src/brain.ts"
      provides: "Brain class refactored as thin wrapper delegating to ProviderManager"
      contains: "ProviderManager"
      min_lines: 60
  key_links:
    - from: "nexus/packages/core/src/providers/gemini.ts"
      to: "@google/generative-ai"
      via: "import GoogleGenerativeAI"
      pattern: "GoogleGenerativeAI"
    - from: "nexus/packages/core/src/providers/manager.ts"
      to: "nexus/packages/core/src/providers/claude.ts"
      via: "registers ClaudeProvider as primary"
      pattern: "ClaudeProvider"
    - from: "nexus/packages/core/src/providers/manager.ts"
      to: "nexus/packages/core/src/providers/gemini.ts"
      via: "registers GeminiProvider as fallback"
      pattern: "GeminiProvider"
    - from: "nexus/packages/core/src/brain.ts"
      to: "nexus/packages/core/src/providers/manager.ts"
      via: "delegates all chat/stream/think calls to ProviderManager"
      pattern: "this\\.manager\\."
    - from: "nexus/packages/core/src/brain.ts"
      to: "existing callers (agent.ts, daemon.ts, api.ts)"
      via: "preserves identical public API: chat(), chatStream(), think(), selectTier()"
      pattern: "export class Brain"
---

<objective>
Extract GeminiProvider from current Brain class, create ProviderManager with fallback logic, and refactor Brain as a thin wrapper. This completes the provider abstraction -- after this plan, Claude is the primary AI and Gemini is the automatic fallback.

Purpose: This is the integration plan that wires everything together. GeminiProvider preserves all existing Gemini functionality. ProviderManager implements the fallback chain (Claude -> Gemini on 429/503/timeout). Brain becomes a thin wrapper so ALL existing callers (agent.ts, daemon.ts, router.ts, api.ts, heartbeat-runner.ts, skill-loader.ts, skill-generator.ts) continue to work without any import changes.

Output: Working multi-provider system where Claude is primary and Gemini is fallback, with zero changes needed to any caller of the Brain class.
</objective>

<execution_context>
@C:\Users\hello\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\hello\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/research/ARCHITECTURE.md
@.planning/research/PITFALLS.md

# Plan 01 and 02 outputs
@.planning/phases/v1.5-01-provider-abstraction-claude/v1.5-01-01-SUMMARY.md
@.planning/phases/v1.5-01-provider-abstraction-claude/v1.5-01-02-SUMMARY.md

# Current Brain class (will be refactored)
@nexus/packages/core/src/brain.ts

# Entry point (wiring -- to understand how Brain is instantiated)
@nexus/packages/core/src/index.ts

# Provider types and ClaudeProvider from prior plans
@nexus/packages/core/src/providers/types.ts
@nexus/packages/core/src/providers/normalize.ts
@nexus/packages/core/src/providers/claude.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extract GeminiProvider from Brain class</name>
  <files>nexus/packages/core/src/providers/gemini.ts</files>
  <action>
Create `nexus/packages/core/src/providers/gemini.ts` by extracting the Gemini-specific code from `brain.ts`.

**Imports:**
```typescript
import { GoogleGenerativeAI } from '@google/generative-ai';
import type { Redis } from 'ioredis';
import type { AIProvider, ProviderChatOptions, ProviderChatResult, ProviderStreamResult, ProviderStreamChunk, ModelTier } from './types.js';
import { prepareForProvider } from './normalize.js';
import { logger } from '../logger.js';
import { retryAsync, type RetryOptions } from '../infra/retry.js';
import { isRetryableError, isRateLimitError, extractRetryAfter, formatErrorMessage } from '../infra/errors.js';
import { BACKOFF_POLICIES } from '../infra/backoff.js';
```

**GEMINI_MODELS mapping** (copy from existing brain.ts):
```typescript
const GEMINI_MODELS: Record<string, string> = {
  flash: 'gemini-3-flash-preview',
  haiku: 'gemini-3-flash-preview',
  sonnet: 'gemini-3-flash-preview',
  opus: 'gemini-3-pro-preview',
};
```

**Class structure:**
```typescript
export class GeminiProvider implements AIProvider {
  readonly id = 'gemini';
  readonly supportsVision = true;
  readonly supportsToolCalling = false; // LivOS uses JSON-in-text, not native Gemini function calling

  private gemini: GoogleGenerativeAI | null = null;
  private redis: Redis | null = null;
  private cachedApiKey: string = '';

  constructor(redis?: Redis) { ... }
```

**Extract these methods from current brain.ts:**

1. `getClient()` (was `getGeminiClient()`) -- same logic:
   - Read from Redis `livos:config:gemini_api_key`
   - Fallback to `process.env.GEMINI_API_KEY`
   - Cache client, re-create on key change

2. `chat(options: ProviderChatOptions): Promise<ProviderChatResult>`
   - Same logic as current `Brain.chat()` but:
   - Use `prepareForProvider(options.messages, 'gemini')` for message conversion instead of inline mapping
   - Return `{ text, inputTokens, outputTokens, provider: 'gemini', model: modelName }`

3. `chatStream(options: ProviderChatOptions): ProviderStreamResult`
   - Same logic as current `Brain.chatStream()` but:
   - Use `prepareForProvider()` for message conversion
   - Return `{ stream: generator, getUsage, provider: 'gemini', model: modelName }`
   - Copy the exact retry-within-stream pattern from brain.ts lines 184-253: the `while (retryCount < maxRetries)` loop inside the async generator, with `retryCount` tracking, exponential backoff via `Math.pow(2, retryCount - 1)`, `isRetryableError()` check on caught errors, `extractRetryAfter()` for server-specified delays, `Math.min(baseDelay, maxDelayMs)` capping, and `result.response.catch(() => {})` to suppress unhandled rejections on the parallel response promise. This is a line-by-line copy of the manual retry-in-generator approach -- do NOT simplify or restructure it.
   - Use existing infra: import `retryAsync`, `isRetryableError`, `isRateLimitError`, `extractRetryAfter`, `formatErrorMessage` from `../infra/errors.js` and `../infra/retry.js`, `BACKOFF_POLICIES` from `../infra/backoff.js`

4. `think(options)` -- same as current `Brain.think()`:
   - Maps to geminiCall internally
   - Uses retryAsync wrapper

5. `isAvailable()` -- check if Gemini API key exists (Redis then env)

6. `getModels()` -- return GEMINI_MODELS

7. Private `geminiCall()` -- extract from current `Brain.geminiCall()`

The implementation should be nearly identical to the current brain.ts Gemini code, just reorganized into the AIProvider interface shape with message format conversion delegated to the normalization layer.
  </action>
  <verify>
`cd nexus && npx tsc --noEmit --pretty 2>&1 | head -20` -- no TypeScript errors. `grep 'implements AIProvider' nexus/packages/core/src/providers/gemini.ts` confirms interface implementation. `grep 'GoogleGenerativeAI' nexus/packages/core/src/providers/gemini.ts` confirms Gemini SDK import. `grep 'retryCount' nexus/packages/core/src/providers/gemini.ts` confirms retry-in-generator pattern is preserved.
  </verify>
  <done>
GeminiProvider implements AIProvider with all methods extracted from Brain class. Uses prepareForProvider('gemini') for message conversion. Preserves the exact retry-in-generator pattern from brain.ts (while loop with retryCount, exponential backoff, isRetryableError checks, extractRetryAfter, response.catch suppression). Reads API key from Redis `livos:config:gemini_api_key` with env fallback.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ProviderManager with fallback logic</name>
  <files>nexus/packages/core/src/providers/manager.ts</files>
  <action>
Create `nexus/packages/core/src/providers/manager.ts`:

```typescript
import type { Redis } from 'ioredis';
import type { AIProvider, ProviderChatOptions, ProviderChatResult, ProviderStreamResult, ModelTier } from './types.js';
import { ClaudeProvider } from './claude.js';
import { GeminiProvider } from './gemini.js';
import { logger } from '../logger.js';
```

**Class ProviderManager:**

```typescript
export class ProviderManager {
  private providers: Map<string, AIProvider> = new Map();
  private fallbackOrder: string[] = []; // ['claude', 'gemini']
  private redis: Redis | null = null;

  constructor(redis?: Redis) {
    this.redis = redis || null;
    // Register providers
    const claude = new ClaudeProvider(redis);
    const gemini = new GeminiProvider(redis);
    this.providers.set('claude', claude);
    this.providers.set('gemini', gemini);
    // Default fallback order: Claude primary, Gemini fallback
    this.fallbackOrder = ['claude', 'gemini'];
  }
```

**getPrimary() method:**
- Check Redis key `nexus:config:ai_provider` for user preference (default: 'claude')
- Return the corresponding provider from the map
- If the preferred provider is not available (isAvailable() returns false), fall through to next in chain

**chat(options) method -- WITH fallback:**
```typescript
async chat(options: ProviderChatOptions): Promise<ProviderChatResult> {
  const errors: Array<{ provider: string; error: Error }> = [];

  for (const providerId of this.fallbackOrder) {
    const provider = this.providers.get(providerId);
    if (!provider) continue;

    // Check availability before trying
    const available = await provider.isAvailable();
    if (!available) {
      logger.debug(`ProviderManager: ${providerId} not available, skipping`);
      continue;
    }

    try {
      const result = await provider.chat(options);
      if (errors.length > 0) {
        // Log that we fell back
        logger.warn('ProviderManager: fell back to alternate provider', {
          primary: errors[0]?.provider,
          fallback: providerId,
          reason: errors[0]?.error?.message,
        });
      }
      return result;
    } catch (err: any) {
      const isFallbackable = this.isFallbackableError(err);
      errors.push({ provider: providerId, error: err });

      if (isFallbackable) {
        logger.warn(`ProviderManager: ${providerId} failed with fallbackable error, trying next`, {
          error: err.message,
          status: err.status,
        });
        continue;
      }

      // Non-fallbackable error (e.g., 400 bad request, auth error) -- don't try next provider
      throw err;
    }
  }

  // All providers failed
  const lastError = errors[errors.length - 1]?.error;
  throw lastError || new Error('No AI providers available');
}
```

**chatStream(options) method -- WITH fallback and hasYielded enforcement:**
- Same pattern as chat() but for streaming
- If the primary provider's stream fails BEFORE yielding any chunks, fall back to the next provider
- If the stream has already started yielding chunks (hasYielded is true), do NOT fall back -- re-throw immediately
- Implementation approach:
  1. Try primary provider's chatStream()
  2. Wrap in a proxy async generator that catches the first error
  3. If error before first yield, switch to fallback provider
  4. If error after first yield, re-throw (partial response would be confusing)

CRITICAL: The `hasYielded` flag MUST be checked after catching an error. Here's the exact pattern:

```typescript
chatStream(options: ProviderChatOptions): ProviderStreamResult {
  let activeResult: ProviderStreamResult | null = null;
  let usageGetter: (() => { inputTokens: number; outputTokens: number }) | null = null;
  let resolvedProvider = '';
  let resolvedModel = '';
  const self = this;

  async function* generateWithFallback(): AsyncGenerator<ProviderStreamChunk> {
    const errors: Array<{ provider: string; error: Error }> = [];

    for (const providerId of self.fallbackOrder) {
      const provider = self.providers.get(providerId);
      if (!provider) continue;

      const available = await provider.isAvailable();
      if (!available) continue;

      try {
        const result = provider.chatStream(options);
        activeResult = result;
        resolvedProvider = result.provider;
        resolvedModel = result.model;
        usageGetter = result.getUsage;

        let hasYielded = false;
        for await (const chunk of result.stream) {
          hasYielded = true;
          yield chunk;
        }
        return; // Success
      } catch (err: any) {
        // CRITICAL: If we already yielded chunks to the consumer, we cannot
        // fall back -- the consumer has partial data from this provider.
        // Re-throw immediately so the caller (Brain/AgentLoop) handles it.
        if (hasYielded) {
          logger.error('ProviderManager: stream failed after partial delivery, cannot fall back', {
            provider: providerId,
            error: err.message,
          });
          throw err;
        }

        errors.push({ provider: providerId, error: err });
        if (self.isFallbackableError(err)) {
          logger.warn(`ProviderManager: ${providerId} stream failed before any chunks, trying next`, {
            error: err.message,
          });
          continue;
        }
        throw err;
      }
    }

    // All providers failed
    const lastError = errors[errors.length - 1]?.error;
    throw lastError || new Error('No AI providers available');
  }

  return {
    stream: generateWithFallback(),
    getUsage: () => usageGetter ? usageGetter() : { inputTokens: 0, outputTokens: 0 },
    provider: resolvedProvider,
    model: resolvedModel,
  };
}
```

Note: The `hasYielded` variable is declared inside the `try` block. To make the check work in the `catch` block, declare `hasYielded` OUTSIDE the try block (before `try {`) so it's accessible in `catch`. The corrected scoping:

```typescript
      let hasYielded = false;
      try {
        const result = provider.chatStream(options);
        // ...
        for await (const chunk of result.stream) {
          hasYielded = true;
          yield chunk;
        }
        return;
      } catch (err: any) {
        if (hasYielded) {
          logger.error('ProviderManager: stream failed after partial delivery, cannot fall back', {
            provider: providerId,
            error: err.message,
          });
          throw err;
        }
        // ... rest of fallback logic
      }
```

**think(options) method:**
- Same fallback pattern as chat() but simpler -- just try providers in order

**isFallbackableError(err) private method:**
- Return true for: 429 (rate limit), 503 (service unavailable), 502 (bad gateway), 529 (overloaded), timeout errors, connection errors
- Return false for: 400 (bad request -- message format issue, don't retry elsewhere), 401 (auth -- won't work on another provider either), 403 (forbidden)
- Check `err.status` (Anthropic SDK sets this), `err.statusCode`, and error message patterns

**getProvider(id) method:**
- Return a specific provider by ID (useful for ProviderManager consumers)

**listProviders() method:**
- Return array of { id, available } for UI/status endpoints

**setFallbackOrder(order: string[]) method:**
- Allow dynamic reordering (for when user changes primary provider via settings)
  </action>
  <verify>
1. `cd nexus && npx tsc --noEmit --pretty 2>&1 | head -20` -- no TypeScript errors
2. `grep 'isFallbackableError' nexus/packages/core/src/providers/manager.ts` confirms fallback error detection
3. `grep -c 'fallbackOrder' nexus/packages/core/src/providers/manager.ts` shows multiple references to fallback chain
4. `grep 'hasYielded' nexus/packages/core/src/providers/manager.ts` confirms the flag exists AND is checked in the catch block
5. `grep 'cannot fall back' nexus/packages/core/src/providers/manager.ts` confirms the partial-delivery guard is present
  </verify>
  <done>
ProviderManager registers Claude (primary) and Gemini (fallback). chat() and chatStream() try Claude first, fall back to Gemini on 429/503/timeout. chatStream() tracks hasYielded flag and enforces no-fallback-after-partial-delivery: if hasYielded is true when an error occurs, it logs an error and re-throws immediately instead of trying the next provider. isFallbackableError() distinguishes retryable errors from non-retryable ones.
  </done>
</task>

<task type="auto">
  <name>Task 3: Refactor Brain as thin wrapper and update wiring</name>
  <files>nexus/packages/core/src/brain.ts, nexus/packages/core/src/providers/index.ts, nexus/packages/core/src/index.ts</files>
  <action>
**Step 1: Refactor brain.ts**

Replace the entire Gemini-specific implementation with delegation to ProviderManager. The PUBLIC API must remain identical so no callers need changes.

The refactored `brain.ts` should look like:

```typescript
import type { Redis } from 'ioredis';
import { ProviderManager } from './providers/manager.js';
import type { ProviderStreamChunk } from './providers/types.js';
import { normalizeMessages } from './providers/normalize.js';
import { logger } from './logger.js';

export type ModelTier = 'none' | 'flash' | 'haiku' | 'sonnet' | 'opus';

// KEEP these interfaces IDENTICAL -- they are the public API contract
export interface ChatMessage {
  role: 'user' | 'model';  // Keep 'model' for backward compat -- normalization handles conversion
  text: string;
  images?: Array<{ base64: string; mimeType: string }>;
}

interface ChatOptions {
  systemPrompt: string;
  messages: ChatMessage[];
  tier?: ModelTier;
  maxTokens?: number;
}

interface ChatStreamChunk {
  text: string;
  done: boolean;
}

interface ChatStreamResult {
  stream: AsyncGenerator<ChatStreamChunk>;
  /** Call after stream ends to get final token usage */
  getUsage: () => { inputTokens: number; outputTokens: number };
}

export class Brain {
  private manager: ProviderManager;

  constructor(redis?: Redis) {
    this.manager = new ProviderManager(redis);
  }

  async think(options: { prompt: string; systemPrompt?: string; tier?: ModelTier; maxTokens?: number }): Promise<string> {
    if (options.tier === 'none') return '';
    return this.manager.think(options);
  }

  async chat(options: ChatOptions): Promise<{ text: string; inputTokens: number; outputTokens: number }> {
    const normalized = normalizeMessages(options.messages);
    const result = await this.manager.chat({
      systemPrompt: options.systemPrompt,
      messages: normalized,
      tier: options.tier,
      maxOutputTokens: options.maxTokens,
    });
    return { text: result.text, inputTokens: result.inputTokens, outputTokens: result.outputTokens };
  }

  chatStream(options: ChatOptions): ChatStreamResult {
    const normalized = normalizeMessages(options.messages);
    const result = this.manager.chatStream({
      systemPrompt: options.systemPrompt,
      messages: normalized,
      tier: options.tier,
      maxOutputTokens: options.maxTokens,
    });
    return {
      stream: result.stream,
      getUsage: result.getUsage,
    };
  }

  selectTier(intentType: string): ModelTier {
    // This is pure logic, no provider dependency -- keep as-is
    const noAi = ['docker_command', 'file_read', 'cron_set', 'status_check', 'direct_execute', 'shell_command', 'system_monitor', 'file_operation', 'service_management'];
    if (noAi.includes(intentType)) return 'none';
    const cheap = ['classify', 'summarize', 'parse_message', 'simple_answer', 'format'];
    if (cheap.includes(intentType)) return 'flash';
    const mid = ['research', 'analyze', 'code_review', 'write_content', 'debug'];
    if (mid.includes(intentType)) return 'sonnet';
    const strong = ['complex_plan', 'architecture', 'multi_step_reasoning'];
    if (strong.includes(intentType)) return 'opus';
    return 'flash';
  }

  /** Access the underlying ProviderManager for advanced usage */
  getProviderManager(): ProviderManager {
    return this.manager;
  }
}
```

CRITICAL: The `ChatMessage` interface still uses `role: 'user' | 'model'` for backward compatibility with all existing callers (agent.ts pushes `role: 'model'`). The `normalizeMessages()` call in chat() and chatStream() converts 'model' to 'assistant' before passing to ProviderManager. This preserves the entire existing codebase without any changes to callers.

CRITICAL: Remove the `GoogleGenerativeAI` import from brain.ts. Remove the private `gemini`, `redis`, `cachedApiKey` fields. Remove `getGeminiClient()`, `geminiCall()` private methods. All Gemini code is now in GeminiProvider.

**Step 2: Update providers/index.ts barrel**

```typescript
export * from './types.js';
export * from './normalize.js';
export { ClaudeProvider } from './claude.js';
export { GeminiProvider } from './gemini.js';
export { ProviderManager } from './manager.js';
```

**Step 3: Verify index.ts needs no changes**

Check `nexus/packages/core/src/index.ts`. The Brain constructor receives `redis` -- this is still the case. `new Brain(redis)` still works because Brain's constructor passes redis to ProviderManager. No changes needed to index.ts.

However, verify all other files that import from brain.ts still compile:
- `agent.ts` imports `{ Brain, ChatMessage }` -- still exported, still same shape
- `daemon.ts` imports `{ Brain, type ModelTier }` -- still exported
- `router.ts` imports `{ Brain, ModelTier }` -- still exported
- `api.ts` imports `{ Brain }` -- still exported
- `heartbeat-runner.ts` imports `type { Brain }` -- still exported
- `commands.ts` imports `type { ModelTier }` -- still exported
- `user-session.ts` imports `type { ModelTier }` -- still exported
- `skill-generator.ts` imports `{ Brain }` -- still exported
- `skill-loader.ts` imports `type { Brain }` -- still exported

ALL imports preserved. No caller changes needed.

**Step 4: Full compilation check**

Run `cd nexus && npx tsc --noEmit` and fix any errors. The most likely issues:
1. Missing exports from brain.ts that were previously there (ThinkOptions was private, ChatOptions was private -- these should remain private/not exported since no caller imports them)
2. Type mismatch between Brain.chatStream return type and what agent.ts expects (ChatStreamChunk must have same shape)
  </action>
  <verify>
1. `cd nexus && npx tsc --noEmit --pretty` -- exits with code 0, no errors
2. `grep 'GoogleGenerativeAI' nexus/packages/core/src/brain.ts` -- returns nothing (Gemini code removed)
3. `grep 'ProviderManager' nexus/packages/core/src/brain.ts` -- confirms delegation
4. `grep "role: 'user' | 'model'" nexus/packages/core/src/brain.ts` -- confirms backward compat
5. `grep 'normalizeMessages' nexus/packages/core/src/brain.ts` -- confirms normalization
6. All existing imports from brain.ts still resolve (check each file)
  </verify>
  <done>
Brain is now a thin wrapper around ProviderManager. ChatMessage still uses `role: 'model'` for backward compatibility -- normalizeMessages() converts to 'assistant' at the boundary. All 9 files that import from brain.ts compile without changes. GeminiProvider extracted to providers/gemini.ts. ProviderManager wired with Claude primary, Gemini fallback. Brain.chat(), Brain.chatStream(), Brain.think(), Brain.selectTier() all have identical signatures. No caller modifications needed. The entire nexus package compiles cleanly.
  </done>
</task>

</tasks>

<verification>
1. `cd nexus && npx tsc --noEmit` passes with zero errors
2. Brain class public API is unchanged: chat(), chatStream(), think(), selectTier() have same signatures
3. All 9 files importing from brain.ts compile without modification
4. ProviderManager tries Claude first, falls back to Gemini on 429/503/timeout
5. ProviderManager.chatStream enforces hasYielded check -- no fallback after partial stream delivery
6. GeminiProvider behavior is identical to the old Brain Gemini implementation, including the exact retry-in-generator pattern
7. Brain.chat() and chatStream() call normalizeMessages() to convert 'model' -> 'assistant'
8. providers/index.ts exports: types, normalize, ClaudeProvider, GeminiProvider, ProviderManager
9. `grep -r 'GoogleGenerativeAI' nexus/packages/core/src/brain.ts` returns empty (no Gemini code in Brain)
</verification>

<success_criteria>
- Zero-change migration: all existing callers of Brain work without modification
- Claude is the primary provider (tried first for all requests)
- Gemini is the automatic fallback on 429/503/timeout errors
- Streaming works identically: chatStream returns same AsyncGenerator shape
- chatStream fallback is guarded by hasYielded: partial delivery prevents fallback
- Token usage (inputTokens, outputTokens) reported from whichever provider handled the request
- The codebase compiles cleanly with `npx tsc --noEmit`
- Brain.ChatMessage still uses 'model' role for backward compatibility
- Message normalization converts 'model' to 'assistant' at the provider boundary
- GeminiProvider chatStream preserves the exact while-loop retry pattern from brain.ts
</success_criteria>

<output>
After completion, create `.planning/phases/v1.5-01-provider-abstraction-claude/v1.5-01-03-SUMMARY.md`
</output>
