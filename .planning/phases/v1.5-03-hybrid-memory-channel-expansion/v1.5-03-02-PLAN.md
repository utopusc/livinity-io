---
phase: v1.5-03-hybrid-memory-channel-expansion
plan: 02
type: execute
wave: 2
depends_on: ["v1.5-03-01"]
files_modified:
  - nexus/packages/core/src/daemon.ts
  - nexus/packages/memory/src/index.ts
autonomous: true

must_haves:
  truths:
    - "The AI's system prompt includes relevant memories from past conversations, assembled within a token budget"
    - "Memory context is assembled by relevance score, not just recency, with a configurable token limit"
    - "The memory service exposes a /context endpoint that returns pre-assembled memory context for a query"
  artifacts:
    - path: "nexus/packages/memory/src/index.ts"
      provides: "/context endpoint that assembles memories within token budget"
      contains: "/context"
    - path: "nexus/packages/core/src/daemon.ts"
      provides: "Memory context injection into agent system prompt"
      contains: "memoryContext"
  key_links:
    - from: "nexus/packages/core/src/daemon.ts"
      to: "http://localhost:3300/context"
      via: "fetch before agent.run()"
      pattern: "localhost:3300/context"
    - from: "nexus/packages/memory/src/index.ts"
      to: "search + token budget assembly"
      via: "/context endpoint"
      pattern: "tokenBudget"
---

<objective>
Add a memory context assembly endpoint to the memory service and inject relevant memories into the AI agent's system prompt before each conversation, using relevance-scored assembly within a configurable token budget.

Purpose: The AI should recall past knowledge automatically without being asked. Relevant memories should be included in the prompt so the AI "remembers" facts from prior conversations.

Output: New /context endpoint in memory service, and memory context injection in daemon agent flow.
</objective>

<execution_context>
@C:\Users\hello\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\hello\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/v1.5-03-hybrid-memory-channel-expansion/v1.5-03-01-SUMMARY.md
@nexus/packages/memory/src/index.ts
@nexus/packages/core/src/daemon.ts
@nexus/packages/core/src/brain.ts
@nexus/packages/core/src/agent.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add /context endpoint for token-budgeted memory assembly</name>
  <files>nexus/packages/memory/src/index.ts</files>
  <action>
Add a new POST `/context` endpoint to the memory service that assembles relevant memories into a formatted context string, respecting a token budget.

**Endpoint: POST /context**

Request body:
```typescript
{
  userId: string;       // Required
  query: string;        // Required — the user's current message, used for relevance scoring
  tokenBudget?: number; // Optional, default 2000 (~1500 words). Max tokens for memory context
  limit?: number;       // Optional, default 20. Max memories to consider
}
```

Implementation:
1. Call the existing search logic internally (do not make HTTP call — extract the search logic into a helper function `searchMemories(userId, query, limit)` that both `/search` and `/context` can use). The helper should return the same scored results (with time-decay) as the /search endpoint.

2. Estimate tokens per memory using a simple heuristic: `Math.ceil(content.length / 4)` (roughly 4 chars per token).

3. Greedily assemble memories by score (highest first) until the token budget is exhausted:
```typescript
let usedTokens = 0;
const selected: Array<{ content: string; score: number; createdAt: number }> = [];
for (const mem of scoredMemories) {
  const tokens = Math.ceil(mem.content.length / 4);
  if (usedTokens + tokens > tokenBudget) break;
  selected.push(mem);
  usedTokens += tokens;
}
```

4. Format the output as a structured context block:
```typescript
const contextBlock = selected.length > 0
  ? `## Known Facts (from memory)\n${selected.map((m, i) => `- ${m.content}`).join('\n')}\n`
  : '';
```

5. Return:
```json
{
  "context": "<formatted context string>",
  "memoriesUsed": 5,
  "tokensUsed": 1234,
  "tokenBudget": 2000
}
```

**Refactoring the search logic:**

Extract the core search logic from the existing `/search` handler into a reusable function:
```typescript
async function searchMemories(
  userId: string,
  query: string,
  limit: number = 10
): Promise<Array<{ id: string; content: string; score: number; createdAt: number; metadata: any }>> {
  // ... existing search logic with embedding + time-decay scoring
}
```

Then update `/search` to call `searchMemories()` and `/context` to also call it. This avoids code duplication.

Do NOT change any existing endpoint behavior. The /search endpoint must return identical results as before.
  </action>
  <verify>
Run `cd nexus/packages/memory && npx tsc --noEmit` to verify TypeScript compiles. Grep for `'/context'` and `searchMemories` and `tokenBudget` to confirm the endpoint exists.
  </verify>
  <done>
Memory service has a /context endpoint that: (1) searches memories by relevance + time-decay, (2) assembles top results within a token budget, (3) returns a formatted "Known Facts" context block. The search logic is shared between /search and /context.
  </done>
</task>

<task type="auto">
  <name>Task 2: Inject memory context into agent system prompt before conversations</name>
  <files>nexus/packages/core/src/daemon.ts</files>
  <action>
Modify the agent processing flow in daemon.ts to fetch relevant memory context before running the agent, and inject it into the system prompt.

**Find the agent handler** (the `router.register('agent', ...)` block around line 864). Before the AgentLoop is created and run, add a memory context fetch:

```typescript
// Fetch relevant memory context for this conversation
let memoryContext = '';
try {
  const memRes = await fetch('http://localhost:3300/context', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'X-API-Key': process.env.LIV_API_KEY || '',
    },
    body: JSON.stringify({
      userId: intent.source === 'web' ? 'default' : (intent.params?.from || 'default'),
      query: task.slice(0, 500), // Use current message for relevance
      tokenBudget: 2000,
      limit: 20,
    }),
  });
  if (memRes.ok) {
    const memData = await memRes.json() as { context: string; memoriesUsed: number };
    if (memData.context && memData.memoriesUsed > 0) {
      memoryContext = memData.context;
      logger.debug('Memory context injected', { memoriesUsed: memData.memoriesUsed });
    }
  }
} catch {
  // Memory service might be down — continue without context
}
```

**Inject the memory context into the task string** that gets passed to the AgentLoop. Find where `task` is assembled (there is already history injection around line 869-871: `if (intent.params.__history) { task = ... }`). After the history injection, add:

```typescript
if (memoryContext) {
  task = `${memoryContext}\n${task}`;
}
```

This places the memory context BEFORE the user's message so the AI sees it as background knowledge.

**Also inject memory context for subagent execution.** Find the `executeSubagentTask` method (around line 2207). Add memory context fetch before the agent runs, and inject it into `contextPrefix`:

```typescript
// In executeSubagentTask, before creating the AgentLoop:
let memoryContext = '';
try {
  const memRes = await fetch('http://localhost:3300/context', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'X-API-Key': process.env.LIV_API_KEY || '',
    },
    body: JSON.stringify({
      userId: 'default',
      query: task.slice(0, 500),
      tokenBudget: 1000, // Smaller budget for subagents
      limit: 10,
    }),
  });
  if (memRes.ok) {
    const memData = await memRes.json() as { context: string; memoriesUsed: number };
    if (memData.context && memData.memoriesUsed > 0) {
      memoryContext = memData.context;
    }
  }
} catch { /* memory service might be down */ }

// Add to contextPrefix
if (memoryContext) {
  contextPrefix += `${memoryContext}\n\n`;
}
```

**Important constraints:**
- Memory fetch is best-effort — if the service is down, continue without context (catch block with no rethrow).
- Do NOT make the memory fetch blocking for the response — if it takes >2s, the agent should proceed anyway. Wrap in a Promise.race with a timeout:
  ```typescript
  const memoryFetchWithTimeout = Promise.race([
    fetch('http://localhost:3300/context', { ... }),
    new Promise<Response>((_, reject) => setTimeout(() => reject(new Error('timeout')), 2000)),
  ]);
  ```
- Use `userId: intent.params?.from || 'default'` to ensure per-user memory isolation for channel users.
- Keep the existing code structure — inject memory, don't restructure the agent handler.
  </action>
  <verify>
Run `cd nexus/packages/core && npx tsc --noEmit` to verify TypeScript compiles. Grep for `localhost:3300/context` and `memoryContext` in daemon.ts to confirm injection points.
  </verify>
  <done>
Before each agent conversation, the daemon fetches relevant memories from the /context endpoint (with 2s timeout, best-effort) and injects them as "Known Facts" into the task context. The AI now automatically recalls past knowledge. Per-user memory isolation is maintained via userId. Subagents also receive memory context with a smaller token budget.
  </done>
</task>

</tasks>

<verification>
1. `cd nexus/packages/memory && npx tsc --noEmit` — zero errors
2. `cd nexus/packages/core && npx tsc --noEmit` — zero errors
3. Grep confirms `/context` endpoint in memory service
4. Grep confirms `searchMemories` helper function exists
5. Grep confirms `memoryContext` injection in daemon agent handler
6. Grep confirms memory context in `executeSubagentTask`
7. Grep confirms 2000ms timeout on memory fetch
</verification>

<success_criteria>
- Memory service /context endpoint returns relevance-scored, token-budgeted memory context
- Agent handler fetches memory context before running the agent
- Memory context appears in agent's input as "Known Facts"
- Subagents also receive memory context (smaller budget)
- Memory fetch failure does not block response delivery (2s timeout, best-effort)
- All TypeScript compiles cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/v1.5-03-hybrid-memory-channel-expansion/v1.5-03-02-SUMMARY.md`
</output>
